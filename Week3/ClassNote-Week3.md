# Week3

[toc]

# Classification and Representation

## Classification

Explain why the linear regression is unsuitable for classification problems.

To attempt classification, one method is to use linear regression and map all predictions greater than 0.5 and all less than 0.5 as a 0. However, this method doesn't work well because classification is not actually a linear function.

Apply LinearRegression to a classification problem is not a great idea.

![Week3/Screen_Shot_2020-11-12_at_3.06.49_PM.png](Week3/Screen_Shot_2020-11-12_at_3.06.49_PM.png)

The classification problem is just like the regression problem, except that the values we now want to predict take on a small number of discrete values. For now, we will focus on the binary classification problem in which y can take on only two values, 0 and 1.

![Week3/Screen_Shot_2020-11-12_at_3.16.32_PM.png](Week3/Screen_Shot_2020-11-12_at_3.16.32_PM.png)

So we develop an algorithm called **Logistic Regression.** Which has the property the output of the predictions of regression are always between zero and one.

## Hypothesis Representation

![Week3/Screen_Shot_2020-11-12_at_3.36.19_PM.png](Week3/Screen_Shot_2020-11-12_at_3.36.19_PM.png)

![Week3/Screen_Shot_2020-11-12_at_3.35.47_PM.png](Week3/Screen_Shot_2020-11-12_at_3.35.47_PM.png)

 

**Summary**

![Week3/Screen_Shot_2020-11-12_at_3.44.49_PM.png](Week3/Screen_Shot_2020-11-12_at_3.44.49_PM.png)

Chineseï¼š

Logistic å‡½æ•°å¯ä»¥çœ‹æˆæ˜¯ä¸€ä¸ªâ€œæŒ¤å‹â€å‡½æ•°ï¼ŒæŠŠä¸€ä¸ªå®æ•°åŸŸçš„è¾“å…¥â€œæŒ¤å‹â€åˆ°
(0, 1).å½“è¾“å…¥å€¼åœ¨ 0 é™„è¿‘æ—¶ï¼ŒSigmoid å‹å‡½æ•°è¿‘ä¼¼ä¸ºçº¿æ€§å‡½æ•°;å½“è¾“å…¥å€¼é è¿‘ä¸¤ç«¯
æ—¶ï¼Œå¯¹è¾“å…¥è¿›è¡ŒæŠ‘åˆ¶.è¾“å…¥è¶Šå°ï¼Œè¶Šæ¥è¿‘äº 0;è¾“å…¥è¶Šå¤§ï¼Œè¶Šæ¥è¿‘äº 1.è¿™æ ·çš„ç‰¹ç‚¹
ä¹Ÿå’Œç”Ÿç‰©ç¥ç»å…ƒç±»ä¼¼ï¼Œå¯¹ä¸€äº›è¾“å…¥ä¼šäº§ç”Ÿå…´å¥‹(è¾“å‡ºä¸º 1)ï¼Œå¯¹å¦ä¸€äº›è¾“å…¥äº§ç”ŸæŠ‘
åˆ¶(è¾“å‡ºä¸º 0).å’Œæ„ŸçŸ¥å™¨ä½¿ç”¨çš„é˜¶è·ƒæ¿€æ´»å‡½æ•°ç›¸æ¯”ï¼ŒLogistic å‡½æ•°æ˜¯è¿ç»­å¯å¯¼çš„ï¼Œ
å…¶æ•°å­¦æ€§è´¨æ›´å¥½.

å› ä¸º Logistic å‡½æ•°çš„æ€§è´¨ï¼Œä½¿å¾—è£…å¤‡äº† Logistic æ¿€æ´»å‡½æ•°çš„ç¥ç»å…ƒå…·æœ‰ä»¥ä¸‹
ä¸¤ç‚¹æ€§è´¨:

1)å…¶è¾“å‡ºç›´æ¥å¯ä»¥çœ‹ä½œæ¦‚ç‡åˆ†å¸ƒï¼Œä½¿å¾—ç¥ç»ç½‘ç»œå¯ä»¥æ›´å¥½åœ°å’Œç»Ÿè®¡å­¦ä¹ æ¨¡å‹è¿›è¡Œç»“åˆ.

2)å…¶å¯ä»¥çœ‹ä½œä¸€ä¸ªè½¯æ€§é—¨(Soft Gate)ï¼Œç”¨æ¥æ§åˆ¶å…¶ä»–ç¥ç»å…ƒè¾“å‡ºä¿¡æ¯çš„æ•°é‡.

## Decision Boundary

Give us a better sense of what the logistic regressions hypothesis function is computing.

![Week3/Screen_Shot_2020-11-12_at_4.01.12_PM.png](Week3/Screen_Shot_2020-11-12_at_4.01.12_PM.png)

 

![Week3/Screen_Shot_2020-11-12_at_5.21.46_PM.png](Week3/Screen_Shot_2020-11-12_at_5.21.46_PM.png)

Non-linear decision boundaries

![Week3/Screen_Shot_2020-11-12_at_6.09.37_PM.png](Week3/Screen_Shot_2020-11-12_at_6.09.37_PM.png)

Summary

![Week3/Screen_Shot_2020-11-12_at_6.17.20_PM.png](Week3/Screen_Shot_2020-11-12_at_6.17.20_PM.png)

# Logistic Regression Model

## Cost Function

How do we fit the parameter's theta.

We can't use the same function that we use for linear regression because the Logistic Function will cause the output to be wavy[non-convex],causing many local optima. In other words, it will not be a convex function.

![Week3/Screen_Shot_2020-11-13_at_7.35.20_AM.png](Week3/Screen_Shot_2020-11-13_at_7.35.20_AM.png)

Logistic regression cost function 

![Week3/Screen_Shot_2020-11-13_at_7.55.20_AM.png](Week3/Screen_Shot_2020-11-13_at_7.55.20_AM.png)

![Week3/Screen_Shot_2020-11-13_at_8.00.45_AM.png](Week3/Screen_Shot_2020-11-13_at_8.00.45_AM.png)

![Week3/Screen_Shot_2020-11-13_at_8.12.44_AM.png](Week3/Screen_Shot_2020-11-13_at_8.12.44_AM.png)

quiz:

![Week3/Screen_Shot_2020-11-13_at_8.11.21_AM.png](Week3/Screen_Shot_2020-11-13_at_8.11.21_AM.png)

We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function.

Instead, our cost function for logistic regression looks like:

![Week3/Screen_Shot_2020-11-13_at_8.44.25_AM.png](Week3/Screen_Shot_2020-11-13_at_8.44.25_AM.png)

![Week3/Screen_Shot_2020-11-13_at_8.44.33_AM.png](Week3/Screen_Shot_2020-11-13_at_8.44.33_AM.png)

If our correct answer 'y' is 0, then the cost function will be 0 if our hypothesis function also outputs 0. If our hypothesis approaches 1, then the cost function will approach infinity.

If our correct answer 'y' is 1, then the cost function will be 0 if our hypothesis function outputs 1. If our hypothesis approaches 0, then the cost function will approach infinity.

Note that writing the cost function in this way guarantees that J(Î¸) is convex for logistic regression.

## Simplified Cost Function and Gradient Descent

![Week3/Screen_Shot_2020-11-13_at_9.09.11_AM.png](Week3/Screen_Shot_2020-11-13_at_9.09.11_AM.png)

![Week3/Screen_Shot_2020-11-13_at_9.11.17_AM.png](Week3/Screen_Shot_2020-11-13_at_9.11.17_AM.png)

We can fully write out our entire cost function as follows:

![Week3/Screen_Shot_2020-11-13_at_9.11.39_AM.png](Week3/Screen_Shot_2020-11-13_at_9.11.39_AM.png)

Notice that this algorithm is identical to the one we used in linear regression. We still have to simultaneously update all values in theta.

![Week3/Screen_Shot_2020-11-13_at_9.32.43_AM.png](Week3/Screen_Shot_2020-11-13_at_9.32.43_AM.png)

![Week3/Screen_Shot_2020-11-13_at_9.20.36_AM.png](Week3/Screen_Shot_2020-11-13_at_9.20.36_AM.png)

## Advanced Optimization

![Week3/Screen_Shot_2020-11-13_at_10.55.22_AM.png](Week3/Screen_Shot_2020-11-13_at_10.55.22_AM.png)

![Week3/Screen_Shot_2020-11-13_at_10.56.39_AM.png](Week3/Screen_Shot_2020-11-13_at_10.56.39_AM.png)

Summary:

"Conjugate gradient", "BFGS", and "L-BFGS" are more sophisticated, faster ways to optimize Î¸ that can be used instead of gradient descent. We suggest that you should not write these more sophisticated algorithms yourself (unless you are an expert in numerical computing) but use the libraries instead, as they're already tested and highly optimized. Octave provides them.

![Week3/Screen_Shot_2020-11-13_at_10.59.34_AM.png](Week3/Screen_Shot_2020-11-13_at_10.59.34_AM.png)

## Multiclass Classification: One-vs-all

How to get Logistic Regression to work for Multi-class classification: One-vs-all

Now we will approach the classification of data when we have more than two categories. Instead of y = {0, 1}, we will expend our defination so that y = {0, 1, 2...n}.

Since y = {0, 1... n} we divide our problem into n+1 (+1 because the index starts at 0) binary classification problems; in each one, we predict the probability that 'y' is a member of one of our classes.

# The Problem of Overfitting

Overfitting: **if we have too many features, the learned hypothesis may fit the training set very well, but fail to generalize to new examples(predict prices on new examples.**

Addressing overfitting:
1.Reduce number of features.

- Manually select which features to keep.
- Model selection algorithm

2.Regularization

- Keep all the features, but reduce magnitude \ values of parameters theta J.
- Works well when we have a lot of features, each of which contributes a bit to predicting y.

## Regularization Cost function

Summary 

If we have overfitting from our hypothesis function, we can reduce the weight that some of the terms in our function carry by increasing their cost.

Say we wnated to make the following function more quadratic[äºŒæ¬¡çš„]:

![Week3/Screen_Shot_2020-11-13_at_1.35.41_PM.png](Week3/Screen_Shot_2020-11-13_at_1.35.41_PM.png)

## Regularized linear regression

![Week3/Screen_Shot_2020-11-13_at_1.59.43_PM.png](Week3/Screen_Shot_2020-11-13_at_1.59.43_PM.png)

![Week3/Screen_Shot_2020-11-13_at_2.02.25_PM.png](Week3/Screen_Shot_2020-11-13_at_2.02.25_PM.png)

We can apply regularization to both linear regression and logistic regression. We will approach linear regression first.

**Gradient Descent:**

![Week3/Screen_Shot_2020-11-13_at_2.41.28_PM.png](Week3/Screen_Shot_2020-11-13_at_2.41.28_PM.png)

**Normal Equation**

![Week3/Screen_Shot_2020-11-13_at_2.41.50_PM.png](Week3/Screen_Shot_2020-11-13_at_2.41.50_PM.png)

## ğŸŒŸRegularized Logistic Regression

The regularized can help solve the overfitting problems.

We can regularize logistic regression in a similar way that we regularize linear regression. As a result, we can avoid overfitting. The following image shows how the regularized function represented by the blue line:

![Week3/Screen_Shot_2020-11-13_at_2.45.17_PM.png](Week3/Screen_Shot_2020-11-13_at_2.45.17_PM.png)

Quiz:

![Week3/Screen_Shot_2020-11-13_at_2.30.28_PM.png](Week3/Screen_Shot_2020-11-13_at_2.30.28_PM.png)

